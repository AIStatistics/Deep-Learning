{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded from [here](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset). as suggested by smerity, we only add the eos flag at each end of sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'\n",
    "PATH=Path('../data/wikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    tokens = []\n",
    "    with open(PATH/filename, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tokens.append(line.split() + [EOS])\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tok = read_file('wiki.train.tokens')\n",
    "val_tok = read_file('wiki.valid.tokens')\n",
    "tst_tok = read_file('wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760, 4358)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_tok), len(val_tok), len(tst_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we numericalize the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(word for sent in trn_tok for word in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 113161),\n",
       " (',', 99913),\n",
       " ('.', 73388),\n",
       " ('of', 56889),\n",
       " ('<unk>', 54625),\n",
       " ('and', 50603),\n",
       " ('in', 39453),\n",
       " ('to', 39190),\n",
       " ('<eos>', 36718),\n",
       " ('a', 34237)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in cnt.most_common()]\n",
    "itos.insert(0,'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos); vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda : 5, {w:i for i,w in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ids = np.array([([stoi[w] for w in s]) for s in trn_tok])\n",
    "val_ids = np.array([([stoi[w] for w in s]) for s in val_tok])\n",
    "tst_ids = np.array([([stoi[w] for w in s]) for s in tst_tok])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the usual AWD LSTM with three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "bptt, bs = 70, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training schedule: 1cycle with either a third phase with cosine annealing or linear decay at one hundreth of the lowest lr. The second one seems to be slightly betters, but by a hair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cycle(steps,lr,opt_fn, div,max_mom,min_mom, wd):\n",
    "    return [TrainingPhase(steps[0], opt_fn, lr=(lr/div,lr), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(max_mom,min_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(steps[1], opt_fn, lr=(lr,lr/div), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(\n",
    "                              min_mom,max_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(steps[2], opt_fn, lr=lr/div, lr_decay=DecayType.COSINE, \n",
    "                          momentum=max_mom, wds=wd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cycle_lin(steps,lr,div,max_mom,min_mom, wd):\n",
    "    return [TrainingPhase(epochs=steps[0], opt_fn=optim.SGD, lr=(lr/div,lr), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(max_mom,min_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(epochs=steps[1], opt_fn=optim.SGD, lr=(lr,lr/div), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(min_mom,max_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(epochs=steps[2], opt_fn=optim.SGD, lr=(lr/div,lr/(div*100)), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=max_mom, wds=wd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cycle(steps, lr, opt_fn, div, max_mom, min_mom, wd):\n",
    "    return [TrainingPhase(steps[0], opt_fn, lr=(lr/div,lr), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(max_mom,min_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "            TrainingPhase(steps[1], opt_fn, lr=lr, momentum=min_mom, wds=wd),\n",
    "            TrainingPhase(steps[2], opt_fn, lr=(lr,lr/div), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(min_mom,max_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "            TrainingPhase(steps[3], opt_fn, lr=lr/div, lr_decay=DecayType.COSINE, \n",
    "                          momentum=max_mom, wds=wd)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the evaluation of the model at the end. TextReader is rewritten from the LanguageModelLoader class to have a constant bptt and only one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReader():\n",
    "\n",
    "    def __init__(self, nums, bptt, backwards=False):\n",
    "        self.bptt,self.backwards = bptt,backwards\n",
    "        self.data = self.batchify(nums)\n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            res = self.get_batch(self.i, self.bptt)\n",
    "            self.i += self.bptt\n",
    "            self.iter += 1\n",
    "            yield res\n",
    "\n",
    "    def __len__(self): return self.n // self.bptt \n",
    "\n",
    "    def batchify(self, data):\n",
    "        data = np.array(data)[:,None]\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return T(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source = self.data\n",
    "        seq_len = min(seq_len, len(source) - 1 - i)\n",
    "        return source[i:i+seq_len], source[i+1:i+1+seq_len].view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation without reinitializing the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_validate(model, source, bptt=2000):\n",
    "    data_source = TextReader(source, bptt)\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    for inputs, targets in tqdm(data_source):\n",
    "        outputs, raws, outs = model(V(inputs))\n",
    "        p_vocab = F.softmax(outputs,1)\n",
    "        for i, pv in enumerate(p_vocab):\n",
    "            targ_pred = pv[targets[i]]\n",
    "            total_loss -= torch.log(targ_pred.detach())\n",
    "    mean = total_loss / (bptt * len(data_source))\n",
    "    return mean, np.exp(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(vec, size=vocab_size):\n",
    "    a = torch.zeros(len(vec), size)\n",
    "    for i,v in enumerate(vec):\n",
    "        a[i,v] = 1.\n",
    "    return V(a)\n",
    "\n",
    "def my_cache_pointer(model, source, scale=1., theta = 0.662, lambd = 0.1279, window=200, bptt=2000):\n",
    "    data_source = TextReader(source, bptt)\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    targ_history = None\n",
    "    hid_history = None\n",
    "    for inputs, targets in tqdm(data_source):\n",
    "        outputs, raws, outs = model(V(inputs))\n",
    "        p_vocab = F.softmax(outputs * scale,1)\n",
    "        start = 0 if targ_history is None else targ_history.size(0)\n",
    "        targ_history = one_hot(targets) if targ_history is None else torch.cat([targ_history, one_hot(targets)])\n",
    "        hiddens = raws[-1].squeeze() #results of the last layer + remove the batch size.\n",
    "        hid_history = scale * hiddens if hid_history is None else torch.cat([hid_history, scale * hiddens])\n",
    "        for i, pv in enumerate(p_vocab):\n",
    "            #Get the cached values\n",
    "            p = pv\n",
    "            if start + i > 0:\n",
    "                targ_cache = targ_history[:start+i] if start + i <= window else targ_history[start+i-window:start+i]\n",
    "                hid_cache = hid_history[:start+i] if start + i <= window else hid_history[start+i-window:start+i]\n",
    "                all_dot_prods = torch.mv(theta * hid_cache, hiddens[i])\n",
    "                exp_dot_prods = F.softmax(all_dot_prods).unsqueeze(1)\n",
    "                p_cache = (exp_dot_prods.expand_as(targ_cache) * targ_cache).sum(0).squeeze()\n",
    "                p = (1-lambd) * pv + lambd * p_cache\n",
    "            targ_pred = p[targets[i]]\n",
    "            total_loss -= torch.log(targ_pred.detach())\n",
    "        targ_history = targ_history[-window:]\n",
    "        hid_history = hid_history[-window:]\n",
    "    mean = total_loss / (bptt * len(data_source))\n",
    "    return mean, np.exp(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.rnn_reg import dropout_mask\n",
    "\n",
    "class EmbeddingDropout1(nn.Module):\n",
    "\n",
    "    \"\"\" Rewritten from EmbeddingDropout. \n",
    "    \n",
    "    Does the same thing but accept either a regular input, or an array of the form\n",
    "    [input1, input2, lambda] to mixup.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "\n",
    "    def forward(self, words, dropout=0.1, scale=None):\n",
    "        if dropout:\n",
    "            size = (self.embed.weight.size(0),1)\n",
    "            mask = Variable(dropout_mask(self.embed.weight.data, size, dropout))\n",
    "            masked_embed_weight = mask * self.embed.weight\n",
    "        else: masked_embed_weight = self.embed.weight\n",
    "\n",
    "        if scale: masked_embed_weight = scale * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.embed.padding_idx\n",
    "        if padding_idx is None: padding_idx = -1\n",
    "\n",
    "        if IS_TORCH_04:\n",
    "            #New here: if the input is a list, take the embeddings for the first two args, then mix them up.\n",
    "            if isinstance(words, list):\n",
    "                X1 = F.embedding(words[0], masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "                   self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "                X2 = F.embedding(words[1], masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "                   self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "                lambd = words[2].view(1,-1,1)\n",
    "                X = X1 * lambd + X2 * (1-lambd)\n",
    "            else:\n",
    "                X = F.embedding(words, masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "                   self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "        else:\n",
    "            #New here: if the input is a list, take the embeddings for the first two args, then mix them up.\n",
    "            if isinstance(words, list):\n",
    "                X1 = self.embed._backend.Embedding.apply(words, masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "                   self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "                X2 = self.embed._backend.Embedding.apply(words, masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "                   self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "                lambd = words[2].view(1,-1,1)\n",
    "                X = X1 * lambd + X2 * (1-lambd)\n",
    "            else:\n",
    "                X = self.embed._backend.Embedding.apply(words, masked_embed_weight, padding_idx, self.embed.max_norm,\n",
    "                  self.embed.norm_type, self.embed.scale_grad_by_freq, self.embed.sparse)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder1(nn.Module):\n",
    "\n",
    "    \"\"\"Rewritten from RNN_Encoder to accept multiple inputs for mixup.\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, nhid, nlayers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                nhid (int): number of hidden activation per LSTM layer\n",
    "                nlayers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs, self.qrnn = 1, qrnn\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout1(self.encoder)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .torchqrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else nhid, (nhid if l != nlayers - 1 else emb_sz)//self.ndir,\n",
    "                save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(nlayers)]\n",
    "            if wdrop:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDrop(rnn.linear, wdrop, weights=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else nhid, (nhid if l != nlayers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(nlayers)]\n",
    "            if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.nhid,self.nlayers,self.dropoute = emb_sz,nhid,nlayers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(nlayers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        \n",
    "        sl,bs = input[0].size() if isinstance(input,list) else input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        #New line here: if the 4-th element of the input is 1, then reset the hidden state.\n",
    "        if is_listy(input) and input[3] == 1: self.reset()\n",
    "        with set_grad_enabled(self.training):\n",
    "            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "            raw_output = self.dropouti(emb)\n",
    "            new_hidden,raw_outputs,outputs = [],[],[]\n",
    "            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "                current_input = raw_output\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "                new_hidden.append(new_h)\n",
    "                raw_outputs.append(raw_output)\n",
    "                if l != self.nlayers - 1: raw_output = drop(raw_output)\n",
    "                outputs.append(raw_output)\n",
    "\n",
    "            self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.nhid if l != self.nlayers - 1 else self.emb_sz)//self.ndir\n",
    "        if IS_TORCH_04: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_())\n",
    "        else: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.qrnn: [r.reset() for r in self.rnns]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.nlayers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.nlayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model1(n_tok, emb_sz, nhid, nlayers, pad_token,\n",
    "                 dropout=0.4, dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5, tie_weights=True, qrnn=False, bias=False):\n",
    "    \"\"\"\n",
    "    Same as get_language_model but creates RNN_Encoder1\n",
    "    \"\"\"\n",
    "\n",
    "    rnn_enc = RNN_Encoder1(n_tok, emb_sz, nhid=nhid, nlayers=nlayers, pad_token=pad_token,\n",
    "                 dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop, qrnn=qrnn)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_tok, emb_sz, dropout, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpDataLoader():\n",
    "    \n",
    "    def __init__(self, nums, bs, bptt, n_keep, alpha, backwards=False):\n",
    "        \"\"\"\n",
    "        Create an instance of a mixup dataloader.\n",
    "        \n",
    "        Args:\n",
    "        nums (np.array): the corpus numericalized\n",
    "        bs (int): batch size\n",
    "        bptt (int): bptt, the number of steps taken into account into backprop\n",
    "        n_keep (int): we reset the model every chunk of n_keep batches\n",
    "        alpha (float): parameter for the beta distribution when picking the lambdas\n",
    "        \"\"\"\n",
    "        self.bs,self.bptt, self.n_keep, self.backwards, self.alpha = bs,bptt,n_keep,backwards,alpha\n",
    "        self.data = self.batchify(nums)\n",
    "        self.n = len(self.data)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        #Shuffle and predraw the pairs of chunks\n",
    "        self.shuffle_chunks()\n",
    "        while self.idx < len(self.chunks1):\n",
    "            #Go through the chunks of batches\n",
    "            self.i = 0\n",
    "            self.len_chunk = self.chunks1[self.idx][1] - self.chunks1[self.idx][0]\n",
    "            while self.i < self.len_chunk:\n",
    "                #Then through the batches\n",
    "                res = self.get_batch() \n",
    "                yield res\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self): \n",
    "        return (self.n-1) // (self.bptt) if (self.n-1) % self.bptt == 0 else (self.n-1) // (self.bptt) + 1\n",
    "\n",
    "    def batchify(self, data):\n",
    "        nb = data.shape[0] // self.bs\n",
    "        data = np.array(data[:nb*self.bs])\n",
    "        data = data.reshape(self.bs, -1).T\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return T(data)\n",
    "    \n",
    "    def shuffle_chunks(self):\n",
    "        #Number of chunks: roughly (n-1) / (bptt * n_keep)\n",
    "        n_chunks = (self.n-1) // (self.bptt * self.n_keep) + 1\n",
    "        if (self.n-1) % (self.bptt * self.n_keep) == 0: n_chunks -= 1\n",
    "        n_res = self.n - 1 - (n_chunks-1) * (self.bptt * self.n_keep)\n",
    "        self.chunks1, self.chunks2 = [], []\n",
    "        #Randomly draw where we will pick the chunk of batches with a length lower than the others.\n",
    "        put_res = np.random.randint(n_chunks)\n",
    "        start = 0\n",
    "        for k in range(n_chunks):\n",
    "            #Split the data into chunks of n_keep batches. \n",
    "            if k != put_res:\n",
    "                self.chunks1.append([start, start + (self.bptt * self.n_keep)])\n",
    "                start += (self.bptt * self.n_keep)\n",
    "            else:\n",
    "                self.chunks1.append([start, start + n_res])\n",
    "                start += n_res\n",
    "        #Remove the chunk with a length different from the others because it needs to be at the same position\n",
    "        #in our two lists of chunks.\n",
    "        res = self.chunks1.pop(put_res)\n",
    "        #Shuffle the chunks\n",
    "        self.chunks1 = np.random.permutation(self.chunks1)\n",
    "        self.chunks2 = np.random.permutation(self.chunks1)\n",
    "        #Add the one with a lower length at the end.\n",
    "        self.chunks1 = np.concatenate([self.chunks1, np.array([res])])\n",
    "        self.chunks2 = np.concatenate([self.chunks1, np.array([res])])\n",
    "\n",
    "    def get_batch(self):\n",
    "        source, i = self.data, self.i\n",
    "        seq_len = min(bptt, self.chunks1[self.idx][1] - self.chunks1[self.idx][0] - self.i)\n",
    "        if self.i == 0:\n",
    "            #At the beggining of a new chunk, draw the lambdas and a pemutation of the batches.\n",
    "            self.lambd = np.random.beta(self.alpha, self.alpha, self.bs)\n",
    "            self.lambd = to_gpu(VV(self.lambd))\n",
    "            self.shuffle = to_gpu(torch.Tensor(np.random.permutation(range(self.bs))).long())\n",
    "            reinit=True\n",
    "        else: reinit = False\n",
    "        #Start indexes for each chunks.\n",
    "        start1, start2 = self.chunks1[self.idx][0] + i, self.chunks2[self.idx][0] + i\n",
    "        #Input: source1, source2, lambda, reinit\n",
    "        res1 = [source[start1:start1+seq_len], source[start2:start2+seq_len,self.shuffle], self.lambd, np.array(reinit).astype(np.int8)]\n",
    "        #Target: source1 shifted, source2 shifted, lambda\n",
    "        targ1 = source[start1+1:start1+1+seq_len].contiguous().view(-1)\n",
    "        targ2 = source[start1+1:start1+1+seq_len,self.shuffle].contiguous().view(-1)\n",
    "        res2 = [targ1,targ2, self.lambd]\n",
    "        self.i += seq_len\n",
    "        return (res1, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A new loss function that accepts targets of the form target1, target2, lambda\n",
    "    \"\"\"\n",
    "    def __init__(self, crit):\n",
    "        super().__init__()\n",
    "        self.crit = crit()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        if not isinstance(target, list): return self.crit(output, target).mean()\n",
    "        loss1, loss2 = self.crit(output,target[0]), self.crit(output,target[1])\n",
    "        loss1, loss2 = loss1.view(-1,target[2].size(0)), loss2.view(-1,target[2].size(0))\n",
    "        return (loss1 * target[2].unsqueeze(0) + loss2 * (1-target[2].unsqueeze(0))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataloader is a mixup, the validation dataloader a regular one. The parameters that gave he best results are 7/0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = MixUpDataLoader1(np.concatenate(trn_ids), bs, bptt, 7, 0.6)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_ids), bs, bptt)\n",
    "md = LanguageModelData(PATH, 0, vocab_size, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropouts. Using mixup allows us to lower this regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.6,0.4,0.5,0.1,0.2]) #Smerity's dropouts from the github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = drops * 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the parameters are the same, with the exception of weight decay that can be divided by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8,0.99))\n",
    "m = get_language_model1(vocab_size, em_sz, nh, nl, 0,\n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4], bias=True)\n",
    "model = LanguageModel(to_gpu(m))\n",
    "learner = RNN_Learner(md, model, opt_fn=opt_fn)\n",
    "learner.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))\n",
    "learner.metrics = [accuracy]\n",
    "learner.clip=0.12\n",
    "learner.unfreeze()\n",
    "learner.reg_fn=partial(seq2seq_reg, alpha=2, beta=1)\n",
    "wd = 6e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367906930f9d480fae51fc7952f9938c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=90), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      6.626329   6.030295   0.178525  \n",
      "    1      6.275505   5.589827   0.214941                   \n",
      "    2      6.027988   5.325422   0.228585                   \n",
      "    3      5.894091   5.133265   0.237486                   \n",
      "    4      5.759592   4.989663   0.246637                   \n",
      "    5      5.645917   4.893019   0.253144                   \n",
      "    6      5.589076   4.811414   0.257452                   \n",
      "    7      5.516833   4.762544   0.26024                    \n",
      "    8      5.443729   4.710184   0.261971                   \n",
      "    9      5.396975   4.677733   0.265007                   \n",
      "    10     5.379718   4.633137   0.268217                   \n",
      "    11     5.334224   4.610691   0.270078                   \n",
      "    12     5.288874   4.59802    0.269693                   \n",
      "    13     5.265066   4.561562   0.272778                   \n",
      "    14     5.273233   4.562297   0.271641                   \n",
      "    15     5.246758   4.542312   0.272835                   \n",
      "    16     5.216068   4.532898   0.274529                   \n",
      "    17     5.229719   4.532675   0.273187                   \n",
      "    18     5.192524   4.523911   0.274853                   \n",
      "    19     5.187457   4.511364   0.275774                   \n",
      "    20     5.195516   4.499867   0.276583                   \n",
      "    21     5.171666   4.503291   0.275934                   \n",
      "    22     5.177153   4.499515   0.275969                   \n",
      "    23     5.182805   4.512135   0.274146                   \n",
      "    24     5.154941   4.494557   0.277097                   \n",
      "    25     5.184615   4.502384   0.27478                    \n",
      "    26     5.170996   4.490614   0.276997                   \n",
      "    27     5.159199   4.488336   0.27764                    \n",
      "    28     5.174464   4.493053   0.277192                   \n",
      "    29     5.189454   4.50579    0.278318                   \n",
      "    30     5.157193   4.499749   0.277637                   \n",
      "    31     5.184397   4.491399   0.278034                   \n",
      "    32     5.177114   4.493066   0.277881                   \n",
      "    33     5.18206    4.488541   0.277623                   \n",
      "    34     5.178177   4.502771   0.276033                   \n",
      "    35     5.199843   4.500518   0.277119                   \n",
      "    36     5.183623   4.515243   0.275506                   \n",
      "    37     5.189091   4.508058   0.275959                   \n",
      "    38     5.197131   4.502707   0.27669                    \n",
      "    39     5.207819   4.510042   0.27562                    \n",
      "    40     5.210927   4.519082   0.276757                   \n",
      "    41     5.197964   4.514505   0.2768                     \n",
      "    42     5.177861   4.524699   0.277477                   \n",
      "    43     5.185628   4.513407   0.2754                     \n",
      "    44     5.205359   4.496628   0.277902                   \n",
      "    45     5.172986   4.484609   0.27843                    \n",
      "    46     5.164484   4.480022   0.279834                   \n",
      "    47     5.160003   4.455373   0.279106                   \n",
      "    48     5.16558    4.466043   0.279338                   \n",
      "    49     5.138186   4.43981    0.281377                   \n",
      "    50     5.110756   4.432053   0.28153                    \n",
      "    51     5.077271   4.433214   0.2801                     \n",
      "    52     5.064951   4.426848   0.281818                   \n",
      "    53     5.056215   4.415006   0.282988                   \n",
      "    54     5.054645   4.402987   0.284088                   \n",
      "    55     5.028499   4.39995    0.285488                   \n",
      "    56     5.019729   4.400858   0.28399                    \n",
      "    57     4.982164   4.3844     0.284593                   \n",
      "    58     4.979679   4.372939   0.285211                   \n",
      "    59     4.969687   4.368094   0.28614                    \n",
      "    60     4.981891   4.354716   0.287106                   \n",
      "    61     4.911375   4.351488   0.287914                   \n",
      "    62     4.95209    4.345455   0.287359                   \n",
      "    63     4.8968     4.338401   0.288977                   \n",
      "    64     4.920658   4.319134   0.289492                   \n",
      "    65     4.86294    4.315365   0.290067                   \n",
      "    66     4.88185    4.31132    0.290526                   \n",
      "    67     4.823708   4.310569   0.290439                   \n",
      "    68     4.80722    4.303623   0.290308                   \n",
      "    69     4.79474    4.289685   0.292336                   \n",
      "    70     4.807685   4.285055   0.293254                   \n",
      "    71     4.781553   4.278372   0.293524                   \n",
      "    72     4.765378   4.270534   0.29311                    \n",
      "    73     4.745676   4.269522   0.293928                   \n",
      "    74     4.683143   4.273508   0.293781                   \n",
      "    75     4.668473   4.258957   0.294176                   \n",
      "    76     4.67758    4.253821   0.295069                   \n",
      "    77     4.65209    4.246094   0.295221                   \n",
      "    78     4.646799   4.242568   0.295861                   \n",
      "    79     4.609731   4.235199   0.29571                    \n",
      "    80     4.567756   4.236626   0.296649                   \n",
      "    81     4.563991   4.21913    0.298206                   \n",
      "    82     4.549918   4.226132   0.297025                   \n",
      "    83     4.551016   4.218874   0.298011                   \n",
      "    84     4.540176   4.218165   0.298074                   \n",
      "    85     4.531996   4.215041   0.298828                   \n",
      "    86     4.503565   4.213403   0.298601                   \n",
      "    87     4.495955   4.214094   0.298183                   \n",
      "    88     4.428206   4.212115   0.29813                    \n",
      "    89     4.452531   4.212716   0.298399                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.212716070810954, 0.2983988434076309]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-2, 1, cycle_len=90, wds=wd, use_clr_beta=(10,7.5,0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:44<00:00,  2.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\n",
       "  4.2080\n",
       " [torch.cuda.FloatTensor of size () (GPU 0)], \n",
       "  67.2209\n",
       " [torch.FloatTensor of size ()])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_validate(learner.model, np.concatenate(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smerity's best with finetuning: 67.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [07:52<00:00,  4.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\n",
       "  3.9554\n",
       " [torch.cuda.FloatTensor of size () (GPU 0)], \n",
       "  52.2156\n",
       " [torch.FloatTensor of size ()])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cache_pointer(learner.model, np.concatenate(val_ids), window=3785)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smerity's best: 52.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the QRNNs, the best parameters for the mixup dataloader are 8/0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1550,4\n",
    "bptt, bs = 70, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = MixUpDataLoader1(np.concatenate(trn_ids), bs, bptt, 8, 0.4)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_ids), bs, bptt)\n",
    "md = LanguageModelData(PATH, 0, vocab_size, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.4,0.4,0.1,0.1,0.2]) #Smerity's dropouts from the github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = drops * 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8,0.99))\n",
    "m = get_language_model1(vocab_size, em_sz, nh, nl, 0,\n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4], qrnn=True, bias=True)\n",
    "model = LanguageModel(to_gpu(m))\n",
    "learner = RNN_Learner(md, model, opt_fn=opt_fn)\n",
    "learner.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))\n",
    "learner.metrics = [accuracy]\n",
    "learner.clip=0.12\n",
    "learner.unfreeze()\n",
    "learner.reg_fn=partial(seq2seq_reg, alpha=2, beta=1)\n",
    "wd = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a52dda730ac4a95815e791250b0d7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=90), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      6.645041   6.111343   0.168837  \n",
      "    1      6.241145   5.621353   0.209918                   \n",
      "    2      6.006832   5.365736   0.221883                   \n",
      "    3      5.804309   5.169753   0.23302                    \n",
      "    4      5.672886   5.02983    0.243427                   \n",
      "    5      5.572516   4.927614   0.249757                   \n",
      "    6      5.480185   4.840228   0.251659                   \n",
      "    7      5.405964   4.775793   0.256458                   \n",
      "    8      5.323781   4.719518   0.260135                   \n",
      "    9      5.301296   4.684981   0.26099                    \n",
      "    10     5.259113   4.659498   0.262679                   \n",
      "    11     5.24911    4.629782   0.263006                   \n",
      "    12     5.196324   4.599059   0.268283                   \n",
      "    13     5.169601   4.592118   0.269083                   \n",
      "    14     5.131697   4.576978   0.269349                   \n",
      "    15     5.130156   4.57565    0.268847                   \n",
      "    16     5.109228   4.544824   0.270566                   \n",
      "    17     5.08938    4.567359   0.270374                   \n",
      "    18     5.085055   4.557732   0.269943                   \n",
      "    19     5.092073   4.529551   0.269761                   \n",
      "    20     5.050129   4.535354   0.269281                   \n",
      "    21     5.061608   4.534164   0.270514                   \n",
      "    22     5.042799   4.525334   0.271235                   \n",
      "    23     5.093311   4.533935   0.271791                   \n",
      "    24     5.090526   4.504567   0.273157                   \n",
      "    25     5.061009   4.514853   0.272238                   \n",
      "    26     5.06793    4.515023   0.27298                    \n",
      "    27     5.098586   4.519208   0.272268                   \n",
      "    28     5.083001   4.522001   0.272778                   \n",
      "    29     5.083461   4.51691    0.270516                   \n",
      "    30     5.090954   4.52026    0.272058                   \n",
      "    31     5.065465   4.532643   0.269444                   \n",
      "    32     5.079646   4.53429    0.269287                   \n",
      "    33     5.066051   4.534481   0.272314                   \n",
      "    34     5.095714   4.537878   0.271023                   \n",
      "    35     5.091392   4.530611   0.272417                   \n",
      "    36     5.129543   4.542377   0.269377                   \n",
      "    37     5.138392   4.545216   0.269866                   \n",
      "    38     5.165642   4.567279   0.270425                   \n",
      "    39     5.138128   4.557847   0.270514                   \n",
      "    40     5.135564   4.554789   0.269561                   \n",
      "    41     5.138917   4.545438   0.268686                   \n",
      "    42     5.095217   4.536446   0.267453                   \n",
      "    43     5.08435    4.523819   0.273534                   \n",
      "    44     5.136052   4.517914   0.271485                   \n",
      "    45     5.069285   4.496474   0.273144                   \n",
      "    46     5.052997   4.490711   0.273823                   \n",
      "    47     5.062989   4.480856   0.274575                   \n",
      "    48     5.031988   4.488932   0.274731                   \n",
      "    49     5.005546   4.460372   0.276215                   \n",
      "    50     5.013542   4.466787   0.277582                   \n",
      "    51     4.97171    4.446997   0.278847                   \n",
      "    52     4.960214   4.435676   0.279613                   \n",
      "    53     4.921822   4.436192   0.278903                   \n",
      "    54     4.932337   4.409701   0.280189                   \n",
      "    55     4.920174   4.400001   0.28084                    \n",
      "    56     4.898736   4.401653   0.280625                   \n",
      "    57     4.880563   4.377549   0.282495                   \n",
      "    58     4.876972   4.386669   0.279066                   \n",
      "    59     4.832559   4.369939   0.282924                   \n",
      "    60     4.842274   4.366953   0.284977                   \n",
      "    61     4.783952   4.354818   0.285994                   \n",
      "    62     4.78493    4.344323   0.286564                   \n",
      "    63     4.760014   4.33985    0.287427                   \n",
      "    64     4.781122   4.330417   0.287477                   \n",
      "    65     4.705451   4.317031   0.288281                   \n",
      "    66     4.7138     4.316244   0.288743                   \n",
      "    67     4.715738   4.302091   0.288728                   \n",
      "    68     4.679006   4.29817    0.289345                   \n",
      "    69     4.622593   4.287198   0.291783                   \n",
      "    70     4.649553   4.292628   0.290838                   \n",
      "    71     4.61578    4.281664   0.291115                   \n",
      "    72     4.595557   4.274585   0.291354                   \n",
      "    73     4.574758   4.267881   0.292458                   \n",
      "    74     4.539172   4.264382   0.29204                    \n",
      "    75     4.537903   4.259481   0.293912                   \n",
      "    76     4.477458   4.248281   0.294409                   \n",
      "    77     4.477541   4.241148   0.294697                   \n",
      "    78     4.43493    4.24016    0.295156                   \n",
      "    79     4.419557   4.231896   0.296171                   \n",
      "    80     4.404289   4.232758   0.296381                   \n",
      "    81     4.375007   4.231765   0.296446                   \n",
      "    82     4.363857   4.22911    0.296851                   \n",
      "    83     4.308866   4.232265   0.296502                   \n",
      "    84     4.297019   4.228879   0.29685                    \n",
      "    85     4.319711   4.227801   0.297055                   \n",
      "    86     4.31708    4.224775   0.297235                   \n",
      "    87     4.284512   4.224898   0.297354                   \n",
      "    88     4.277074   4.224936   0.297453                   \n",
      "    89     4.26808    4.220731   0.297647                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.220731449127197, 0.29764690498511]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-2, 1, cycle_len=90, wds=wd, use_clr_beta=(10,10,0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:18<00:00,  5.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\n",
       "  4.2194\n",
       " [torch.cuda.FloatTensor of size () (GPU 2)], \n",
       "  67.9927\n",
       " [torch.FloatTensor of size ()])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_validate(learner.model, np.concatenate(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smerity's best with finetuning: 68.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [07:26<00:00,  4.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\n",
       "  3.9860\n",
       " [torch.cuda.FloatTensor of size () (GPU 2)], \n",
       "  53.8380\n",
       " [torch.FloatTensor of size ()])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cache_pointer(learner.model, np.concatenate(val_ids), window=3785)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smerity's best: 53.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
