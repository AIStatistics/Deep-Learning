{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded from [here](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset). as suggested by smerity, we only add the eos flag at each end of sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'\n",
    "PATH=Path('../data/wikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    tokens = []\n",
    "    with open(PATH/filename, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tokens.append(line.split() + [EOS])\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tok = read_file('wiki.train.tokens')\n",
    "val_tok = read_file('wiki.valid.tokens')\n",
    "tst_tok = read_file('wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760, 4358)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_tok), len(val_tok), len(tst_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we numericalize the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(word for sent in trn_tok for word in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 113161),\n",
       " (',', 99913),\n",
       " ('.', 73388),\n",
       " ('of', 56889),\n",
       " ('<unk>', 54625),\n",
       " ('and', 50603),\n",
       " ('in', 39453),\n",
       " ('to', 39190),\n",
       " ('<eos>', 36718),\n",
       " ('a', 34237)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in cnt.most_common()]\n",
    "itos.insert(0,'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos); vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda : 5, {w:i for i,w in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ids = np.array([([stoi[w] for w in s]) for s in trn_tok])\n",
    "val_ids = np.array([([stoi[w] for w in s]) for s in val_tok])\n",
    "tst_ids = np.array([([stoi[w] for w in s]) for s in tst_tok])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the usual AWD LSTM with three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "bptt, bs = 70, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_ids), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_ids), bs, bptt)\n",
    "md = LanguageModelData(PATH, 0, vocab_size, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops_jh = np.array([0.25, 0.1, 0.2, 0.02, 0.15]) #Jeremy's dropouts\n",
    "drops_sm = np.array([0.5,0.4,0.5,0.1,0.3]) #Smerity's dropouts from the paper\n",
    "drops = np.array([0.6,0.4,0.5,0.1,0.2]) #Smerity's dropouts from the github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training schedule: 1cycle with either a third phase with cosine annealing or linear decay at one hundreth of the lowest lr. The second one seems to be slightly betters, but by a hair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cycle(steps,lr,div,max_mom,min_mom, wd):\n",
    "    return [TrainingPhase(epochs=steps[0], opt_fn=optim.SGD, lr=(lr/div,lr), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(max_mom,min_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(epochs=steps[1], opt_fn=optim.SGD, lr=(lr,lr/div), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(min_mom,max_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(epochs=steps[2], opt_fn=optim.SGD, lr=lr/div, lr_decay=DecayType.COSINE, \n",
    "                          momentum=max_mom, wds=wd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cycle_lin(steps,lr,div,max_mom,min_mom, wd):\n",
    "    return [TrainingPhase(epochs=steps[0], opt_fn=optim.SGD, lr=(lr/div,lr), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(max_mom,min_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(epochs=steps[1], opt_fn=optim.SGD, lr=(lr,lr/div), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(min_mom,max_mom), momentum_decay=DecayType.LINEAR, wds=wd),\n",
    "           TrainingPhase(epochs=steps[2], opt_fn=optim.SGD, lr=(lr/div,lr/(div*100)), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=max_mom, wds=wd)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the evaluation of the model at the end. TextReader is rewritten from the LanguageModelLoader class to have a constant bptt and only one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReader():\n",
    "\n",
    "    def __init__(self, nums, bptt, backwards=False):\n",
    "        self.bptt,self.backwards = bptt,backwards\n",
    "        self.data = self.batchify(nums)\n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            res = self.get_batch(self.i, self.bptt)\n",
    "            self.i += self.bptt\n",
    "            self.iter += 1\n",
    "            yield res\n",
    "\n",
    "    def __len__(self): return self.n // self.bptt \n",
    "\n",
    "    def batchify(self, data):\n",
    "        data = np.array(data)[:,None]\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return T(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source = self.data\n",
    "        seq_len = min(seq_len, len(source) - 1 - i)\n",
    "        return source[i:i+seq_len], source[i+1:i+1+seq_len].view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation without reinitializing the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_validate(model, source, bptt=2000):\n",
    "    data_source = TextReader(source, bptt)\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    targ_history = None\n",
    "    hid_history = None\n",
    "    for inputs, targets in tqdm(data_source):\n",
    "        outputs, raws, outs = model(V(inputs))\n",
    "        p_vocab = F.softmax(outputs,1)\n",
    "        for i, pv in enumerate(p_vocab):\n",
    "            targ_pred = pv[targets[i]]\n",
    "            total_loss -= torch.log(targ_pred.detach())\n",
    "    mean = total_loss / (bptt * len(data_source))\n",
    "    return mean, np.exp(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(vec, size=vocab_size):\n",
    "    a = torch.zeros(len(vec), size)\n",
    "    for i,v in enumerate(vec):\n",
    "        a[i,v] = 1.\n",
    "    return V(a)\n",
    "\n",
    "def my_cache_pointer(model, source, theta = 0.662, lambd = 0.1279, window=200, bptt=2000):\n",
    "    data_source = TextReader(source, bptt)\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    targ_history = None\n",
    "    hid_history = None\n",
    "    for inputs, targets in tqdm(data_source):\n",
    "        outputs, raws, outs = model(V(inputs))\n",
    "        p_vocab = F.softmax(outputs,1)\n",
    "        start = 0 if targ_history is None else targ_history.size(0)\n",
    "        targ_history = one_hot(targets) if targ_history is None else torch.cat([targ_history, one_hot(targets)])\n",
    "        hiddens = raws[-1].squeeze() #results of the last layer + remove the batch size.\n",
    "        hid_history = hiddens if hid_history is None else torch.cat([hid_history, hiddens])\n",
    "        for i, pv in enumerate(p_vocab):\n",
    "            #Get the cached values\n",
    "            p = pv\n",
    "            if start + i > 0:\n",
    "                targ_cache = targ_history[:start+i] if start + i <= window else targ_history[start+i-window:start+i]\n",
    "                hid_cache = hid_history[:start+i] if start + i <= window else hid_history[start+i-window:start+i]\n",
    "                all_dot_prods = torch.mv(theta * hid_cache, hiddens[i])\n",
    "                exp_dot_prods = F.softmax(all_dot_prods).unsqueeze(1)\n",
    "                p_cache = (exp_dot_prods.expand_as(targ_cache) * targ_cache).sum(0).squeeze()\n",
    "                p = (1-lambd) * pv + lambd * p_cache\n",
    "            targ_pred = p[targets[i]]\n",
    "            total_loss -= torch.log(targ_pred.detach())\n",
    "        targ_history = targ_history[-window:]\n",
    "        hid_history = hid_history[-window:]\n",
    "    mean = total_loss / (bptt * len(data_source))\n",
    "    return mean, np.exp(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the model with the best set of hyper-parameters I've found:\n",
    " - dropouts from the github repo\n",
    " - grad clipping at 0.25\n",
    " - AR and TAR loss with alpha=2 and beta=1\n",
    " - wd=1.2e-6 \n",
    "Changing any one of those didn't yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.SGD, momentum=0.95)\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl,\n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "learner.metrics = [accuracy]\n",
    "learner.clip=0.12\n",
    "learner.unfreeze()\n",
    "learner.reg_fn=partial(seq2seq_reg, alpha=2, beta=1)\n",
    "wd = 1.2e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7412125ffedf4ed98e13d973c3e7920f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 51/297 [00:09<00:44,  5.55it/s, loss=10.4]\n",
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      28.393529  155.721588 0.053903  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find(wds=wd, end_lr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot(10,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are very robust to large LRs and the gradient clipping will prevent divergence and explosion so we can take a LR near the minimum of the curve (30) (lesser values don't give as good a result). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c04b2af5944072af2613355c9a8825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=90), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      6.628112   6.184458   0.162373  \n",
      "    1      6.153111   5.763566   0.19393                    \n",
      "    2      5.871724   5.499626   0.218413                   \n",
      "    3      5.671414   5.311994   0.226338                   \n",
      "    4      5.514653   5.191954   0.2341                     \n",
      "    5      5.382676   5.100694   0.237727                   \n",
      "    6      5.28028    4.994971   0.245732                   \n",
      "    7      5.197885   4.937758   0.248153                   \n",
      "    8      5.129779   4.89894    0.24881                    \n",
      "    9      5.086409   4.888917   0.246337                   \n",
      "    10     5.022318   4.819836   0.254046                   \n",
      "    11     4.961503   4.778642   0.257679                   \n",
      "    12     4.943135   4.797901   0.251169                   \n",
      "    13     4.893374   4.746737   0.257991                   \n",
      "    14     4.858212   4.733589   0.257597                   \n",
      "    15     4.853794   4.757826   0.256396                   \n",
      "    16     4.829774   4.699786   0.260627                   \n",
      "    17     4.816683   4.698032   0.261095                   \n",
      "    18     4.792389   4.696066   0.259225                   \n",
      "    19     4.77915    4.691632   0.2599                     \n",
      "    20     4.770586   4.693496   0.258266                   \n",
      "    21     4.740906   4.656945   0.261809                   \n",
      "    22     4.745472   4.683056   0.261879                   \n",
      "    23     4.736179   4.689861   0.257528                   \n",
      "    24     4.71485    4.642422   0.260784                   \n",
      "    25     4.702522   4.640438   0.264478                   \n",
      "    26     4.711544   4.670192   0.260692                   \n",
      "    27     4.696842   4.630093   0.262287                   \n",
      "    28     4.698566   4.652778   0.259395                   \n",
      "    29     4.697765   4.649539   0.259032                   \n",
      "    30     4.675126   4.613219   0.263693                   \n",
      "    31     4.651553   4.633019   0.262939                   \n",
      "    32     4.656934   4.630734   0.262286                   \n",
      "    33     4.633595   4.67693    0.256976                   \n",
      "    34     4.619785   4.616129   0.262666                   \n",
      "    35     4.618578   4.631188   0.260656                   \n",
      "    36     4.602003   4.591835   0.266242                   \n",
      "    37     4.588308   4.592105   0.267046                   \n",
      "    38     4.574807   4.562205   0.26683                    \n",
      "    39     4.567198   4.577085   0.266835                   \n",
      "    40     4.559919   4.574068   0.266125                   \n",
      "    41     4.551848   4.548969   0.269177                   \n",
      "    42     4.52185    4.538066   0.270832                   \n",
      "    43     4.524078   4.558314   0.266923                   \n",
      "    44     4.511556   4.549604   0.266736                   \n",
      "    45     4.491623   4.539018   0.267617                   \n",
      "    46     4.506206   4.541572   0.268443                   \n",
      "    47     4.453718   4.523647   0.270518                   \n",
      "    48     4.465969   4.528654   0.269667                   \n",
      "    49     4.429361   4.507297   0.272509                   \n",
      "    50     4.420043   4.498888   0.272415                   \n",
      "    51     4.396214   4.490184   0.273711                   \n",
      "    52     4.386297   4.489014   0.273632                   \n",
      "    53     4.368642   4.496087   0.275629                   \n",
      "    54     4.336937   4.473784   0.275443                   \n",
      "    55     4.357731   4.458461   0.276327                   \n",
      "    56     4.285727   4.4505     0.278376                   \n",
      "    57     4.268256   4.435329   0.278349                   \n",
      "    58     4.260642   4.424965   0.279938                   \n",
      "    59     4.236656   4.415108   0.280714                   \n",
      "    60     4.174799   4.396742   0.282579                   \n",
      "    61     4.171991   4.400411   0.282172                   \n",
      "    62     4.147621   4.38713    0.282438                   \n",
      "    63     4.131076   4.378929   0.284217                   \n",
      "    64     4.123507   4.381564   0.283562                   \n",
      "    65     4.125865   4.373787   0.284536                   \n",
      "    66     4.117826   4.373243   0.283837                   \n",
      "    67     4.087021   4.362433   0.284789                   \n",
      "    68     4.063827   4.362609   0.284747                   \n",
      "    69     4.067848   4.355325   0.285965                   \n",
      "    70     4.050684   4.354864   0.28633                    \n",
      "    71     4.045093   4.355526   0.286265                   \n",
      "    72     4.04821    4.346709   0.28592                    \n",
      "    73     4.019781   4.351393   0.286188                   \n",
      "    74     4.003364   4.338651   0.287468                   \n",
      "    75     3.994394   4.339071   0.286403                   \n",
      "    76     4.002044   4.336948   0.28653                    \n",
      "    77     3.986849   4.334219   0.28619                    \n",
      "    78     4.010163   4.330595   0.287425                   \n",
      "    79     3.957584   4.322871   0.288749                   \n",
      "    80     3.946121   4.32205    0.288278                   \n",
      "    81     3.947767   4.313202   0.289981                   \n",
      "    82     3.949915   4.313076   0.2896                     \n",
      "    83     3.960489   4.310744   0.290152                   \n",
      "    84     3.939803   4.310012   0.289535                   \n",
      "    85     3.921758   4.311282   0.289578                   \n",
      "    86     3.972662   4.303909   0.290421                   \n",
      "    87     3.962061   4.303995   0.290575                   \n",
      "    88     3.957885   4.303369   0.290522                   \n",
      "    89     3.909788   4.300513   0.291127                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.300513040785696, 0.2911274534668408]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit_opt_sched(one_cycle_lin([30,30,30], 30, 10, 0.95, 0.85, wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:39<00:00,  2.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(4.2898, device='cuda:0'), tensor(72.9502))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_validate(learner.model, np.concatenate(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [07:50<00:00,  4.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(4.0113, device='cuda:0'), tensor(55.2198))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cache_pointer(learner.model, np.concatenate(val_ids), window=3785)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we go for a longer cycle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.SGD, momentum=0.95)\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl,\n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "learner.metrics = [accuracy]\n",
    "learner.clip=0.12\n",
    "learner.unfreeze()\n",
    "learner.reg_fn=partial(seq2seq_reg, alpha=2, beta=1)\n",
    "wd = 1.2e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      6.649408   6.197354   0.167872  \n",
      "    1      6.195533   5.783183   0.200387                   \n",
      "    2      5.918022   5.524694   0.215999                   \n",
      "    3      5.690343   5.356249   0.22505                    \n",
      "    4      5.537789   5.202367   0.233575                   \n",
      "    5      5.414285   5.127078   0.235668                   \n",
      "    6      5.307435   5.041664   0.23517                    \n",
      "    7      5.212184   4.963502   0.246997                   \n",
      "    8      5.149351   4.897415   0.248754                   \n",
      "    9      5.090227   4.867299   0.251758                   \n",
      "    10     5.010004   4.825522   0.254741                   \n",
      "    11     4.962746   4.793763   0.256                      \n",
      "    12     4.947663   4.79204    0.254782                   \n",
      "    13     4.897349   4.757676   0.255848                   \n",
      "    14     4.865117   4.72921    0.256547                   \n",
      "    15     4.832319   4.713204   0.259961                   \n",
      "    16     4.819389   4.694065   0.260804                   \n",
      "    17     4.806942   4.723109   0.258918                   \n",
      "    18     4.762922   4.67524    0.262903                   \n",
      "    19     4.755131   4.658179   0.262187                   \n",
      "    20     4.748625   4.677272   0.261479                   \n",
      "    21     4.730597   4.645309   0.264832                   \n",
      "    22     4.714807   4.630424   0.266005                   \n",
      "    23     4.701996   4.663434   0.261883                   \n",
      "    24     4.678897   4.630925   0.263253                   \n",
      "    25     4.685403   4.634238   0.26028                    \n",
      "    26     4.663449   4.634643   0.26322                    \n",
      "    27     4.659912   4.628077   0.259481                   \n",
      "    28     4.65907    4.634602   0.260313                   \n",
      "    29     4.656164   4.619688   0.263539                   \n",
      "    30     4.646468   4.629173   0.262927                   \n",
      "    31     4.635587   4.617103   0.262959                   \n",
      "    32     4.625095   4.610733   0.264643                   \n",
      "    33     4.609065   4.595483   0.266779                   \n",
      "    34     4.609857   4.605245   0.262682                   \n",
      "    35     4.623473   4.615111   0.263825                   \n",
      "    36     4.612255   4.618142   0.263836                   \n",
      "    37     4.59817    4.590389   0.265178                   \n",
      "    38     4.599087   4.61217    0.26373                    \n",
      "    39     4.580744   4.584697   0.265973                   \n",
      "    40     4.580185   4.580826   0.268722                   \n",
      "    41     4.579307   4.588978   0.265159                   \n",
      "    42     4.617223   4.612496   0.26092                    \n",
      "    43     4.58218    4.59744    0.264395                   \n",
      "    44     4.606566   4.584587   0.265671                   \n",
      "    45     4.577      4.590339   0.262883                   \n",
      "    46     4.603337   4.556019   0.26912                    \n",
      "    47     4.582302   4.576079   0.264861                   \n",
      "    48     4.573652   4.609775   0.2594                     \n",
      "    49     4.570653   4.601252   0.261732                   \n",
      "    50     4.55572    4.594507   0.262381                   \n",
      "    51     4.566146   4.564219   0.26538                    \n",
      "    52     4.541175   4.574361   0.266589                   \n",
      "    53     4.556397   4.563991   0.268445                   \n",
      "    54     4.536065   4.56424    0.266827                   \n",
      "    55     4.554233   4.566941   0.265304                   \n",
      "    56     4.511211   4.567695   0.265307                   \n",
      "    57     4.528295   4.54725    0.267549                   \n",
      "    58     4.514465   4.558794   0.266461                   \n",
      "    59     4.511183   4.567937   0.262473                   \n",
      "    60     4.517674   4.5524     0.266339                   \n",
      "    61     4.492527   4.536125   0.266537                   \n",
      "    62     4.490115   4.563895   0.262317                   \n",
      "    63     4.486087   4.527713   0.269725                   \n",
      "    64     4.501113   4.526193   0.270721                   \n",
      "    65     4.47306    4.561641   0.266379                   \n",
      "    66     4.452989   4.52104    0.27085                    \n",
      "    67     4.457737   4.515522   0.269519                   \n",
      "    68     4.458689   4.524951   0.270057                   \n",
      "    69     4.432127   4.52077    0.269101                   \n",
      "    70     4.429818   4.513038   0.272313                   \n",
      "    71     4.410089   4.514795   0.271919                   \n",
      "    72     4.424086   4.499379   0.27156                    \n",
      "    73     4.410259   4.503758   0.271314                   \n",
      "    74     4.413021   4.512941   0.2687                     \n",
      "    75     4.389966   4.515917   0.268959                   \n",
      "    76     4.387676   4.511017   0.268987                   \n",
      "    77     4.376257   4.502197   0.268254                   \n",
      "    78     4.414433   4.507595   0.272182                   \n",
      "    79     4.368787   4.497667   0.27215                    \n",
      "    80     4.359642   4.480374   0.273635                   \n",
      "    81     4.343029   4.480061   0.274905                   \n",
      "    82     4.328357   4.471262   0.275672                   \n",
      "    83     4.314973   4.462957   0.276393                   \n",
      "    84     4.305716   4.473528   0.274497                   \n",
      "    85     4.308005   4.460176   0.277077                   \n",
      "    86     4.315002   4.456      0.276681                   \n",
      "    87     4.278982   4.445131   0.278608                   \n",
      "    88     4.266938   4.447204   0.278355                   \n",
      "    89     4.250668   4.436924   0.277769                   \n",
      "    90     4.249078   4.438421   0.279021                   \n",
      "    91     4.229262   4.4459     0.27499                    \n",
      "    92     4.248421   4.428647   0.278395                   \n",
      "    93     4.204804   4.424638   0.280523                   \n",
      "    94     4.177278   4.424596   0.278264                   \n",
      "    95     4.155129   4.407901   0.281168                   \n",
      "    96     4.129777   4.407362   0.28111                    \n",
      "    97     4.114565   4.401756   0.281119                   \n",
      "    98     4.100095   4.384579   0.281917                   \n",
      "    99     4.073934   4.37695    0.283603                   \n",
      "   100     4.075444   4.378812   0.283572                   \n",
      "   101     4.082457   4.363163   0.284674                   \n",
      "   102     4.051677   4.364062   0.284058                   \n",
      "   103     4.033063   4.365989   0.283371                   \n",
      "   104     4.010315   4.35532    0.283701                   \n",
      "   105     4.025712   4.359907   0.284852                   \n",
      "   106     4.022362   4.350756   0.285899                   \n",
      "   107     3.981305   4.343925   0.286282                   \n",
      "   108     3.972874   4.345254   0.287117                   \n",
      "   109     4.000131   4.347037   0.286597                   \n",
      "   110     3.960816   4.337091   0.287973                   \n",
      "   111     3.963151   4.342388   0.28625                    \n",
      "   112     3.941941   4.337061   0.286238                   \n",
      "   113     3.957989   4.334799   0.286591                   \n",
      "   114     3.969365   4.326601   0.287773                   \n",
      "   115     3.912359   4.332631   0.287288                   \n",
      "   116     3.940265   4.335148   0.286299                   \n",
      "   117     3.918527   4.330644   0.288163                   \n",
      "   118     3.97689    4.32098    0.287452                   \n",
      "   119     3.888983   4.315356   0.289135                   \n",
      "   120     3.908631   4.315285   0.28926                    \n",
      "   121     3.879578   4.32234    0.288383                   \n",
      "   122     3.881356   4.309876   0.289078                   \n",
      "   123     3.887289   4.310107   0.289748                   \n",
      "   124     3.884033   4.311704   0.289798                   \n",
      "   125     3.893852   4.307212   0.290339                   \n",
      "   126     3.87459    4.303376   0.290613                   \n",
      "   127     3.842517   4.305129   0.290059                   \n",
      "   128     3.855131   4.300945   0.290232                   \n",
      "   129     3.885341   4.297263   0.290891                   \n",
      "   130     3.820358   4.298969   0.290194                   \n",
      "   131     3.81189    4.298539   0.289748                   \n",
      "   132     3.830213   4.299474   0.289739                   \n",
      "   133     3.799521   4.295897   0.291061                   \n",
      "   134     3.824245   4.29446    0.290947                   \n",
      "   135     3.821448   4.287996   0.291904                   \n",
      "   136     3.813346   4.288382   0.291501                   \n",
      "   137     3.793428   4.295241   0.291664                   \n",
      "   138     3.79816    4.290579   0.291963                   \n",
      "   139     3.793121   4.2841     0.292176                   \n",
      "   140     3.842335   4.282172   0.292219                   \n",
      "   141     3.776307   4.276905   0.292594                   \n",
      "   142     3.79465    4.279565   0.292575                   \n",
      "   143     3.787246   4.277176   0.29355                    \n",
      "   144     3.761223   4.275956   0.293182                   \n",
      "   145     3.752886   4.278934   0.2933                     \n",
      "   146     3.758388   4.2772     0.293114                   \n",
      "   147     3.758411   4.276733   0.292665                   \n",
      "   148     3.78082    4.271979   0.29329                    \n",
      "   149     3.783453   4.270148   0.293407                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.270147887631959, 0.2934068623857171]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit_opt_sched(one_cycle_lin([50,50,50], 30, 10, 0.95, 0.85, wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:39<00:00,  2.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(4.2590, device='cuda:0'), tensor(70.7395))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_validate(learner.model, np.concatenate(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(smerity is at 68 for the first trainign of a model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [07:53<00:00,  4.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(3.9722, device='cuda:0'), tensor(53.1025))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cache_pointer(learner.model, np.concatenate(val_ids), window=3785)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(smerity is at 52 for the model with cache pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
