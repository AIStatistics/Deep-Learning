{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.models.cifar10.wideresnet import wrn_22\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PATH = Path(\"../data/cifar10/\")\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n",
    "\n",
    "bs=128\n",
    "sz=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutout is already implemented in the fastai library. Args are n_holes (1 in the paper), size of the hole, probability of applying (default: 0.5). Applying Cutout after normalization or before (which means blanking with 0s or the means) doesn't change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomCrop(sz), RandomFlip(), Cutout(1,16)], pad=sz//8)\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b08c78f7f341a9bc973c6d2765dacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.265698   1.131748   0.5859    \n",
      "    1      1.011879   0.925887   0.6743                     \n",
      "    2      0.874358   0.83785    0.7146                      \n",
      "    3      0.756501   0.661968   0.7674                      \n",
      "    4      0.702329   0.576785   0.8056                      \n",
      "    5      0.679879   0.568449   0.8077                      \n",
      "    6      0.624821   0.590153   0.7986                      \n",
      "    7      0.584256   0.636968   0.7912                      \n",
      "    8      0.555692   0.508859   0.8294                      \n",
      "    9      0.521765   0.60031    0.8008                      \n",
      "    10     0.516555   0.498995   0.835                       \n",
      "    11     0.494465   0.593156   0.8126                      \n",
      "    12     0.478316   0.486834   0.8367                      \n",
      "    13     0.453475   0.383599   0.8663                      \n",
      "    14     0.429461   0.397569   0.8682                      \n",
      "    15     0.413949   0.458565   0.8433                      \n",
      "    16     0.393276   0.341568   0.8866                      \n",
      "    17     0.375791   0.33701    0.8912                      \n",
      "    18     0.344201   0.312567   0.8961                      \n",
      "    19     0.30564    0.304323   0.9015                      \n",
      "    20     0.302724   0.277031   0.9075                      \n",
      "    21     0.272228   0.239014   0.9218                      \n",
      "    22     0.254668   0.257381   0.915                       \n",
      "    23     0.234972   0.218338   0.9288                      \n",
      "    24     0.214596   0.223773   0.9261                      \n",
      "    25     0.200005   0.215595   0.9333                      \n",
      "    26     0.178975   0.211857   0.9336                      \n",
      "    27     0.176236   0.209497   0.9337                      \n",
      "    28     0.159292   0.19866    0.9393                      \n",
      "    29     0.140852   0.194932   0.9394                      \n",
      "    30     0.120123   0.193329   0.9411                      \n",
      "    31     0.117817   0.192284   0.9458                      \n",
      "    32     0.104493   0.186788   0.9465                      \n",
      "    33     0.101104   0.185037   0.9481                       \n",
      "\n",
      "CPU times: user 1h 27min 53s, sys: 17min 56s, total: 1h 45min 49s\n",
      "Wall time: 28min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18503666452169418, 0.9481]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=34, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did obtain 95% in 34 epochs, but not too sure with which hyper-parameters. Maybe a bit more wd. Anyway, you get the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpDataLoader(object):\n",
    "    \"\"\"\n",
    "    Creates a new data loader with mixup from a given dataloader.\n",
    "    \n",
    "    Mixup is applied between a batch and a shuffled version of itself. \n",
    "    If we use a regular beta distribution, this can create near duplicates as some lines might be \n",
    "    1 * original + 0 * shuffled while others could be 0 * original + 1 * shuffled, this is why\n",
    "    there is a trick where we take the maximum of lambda and 1-lambda.\n",
    "    \n",
    "    Arguments:\n",
    "    dl (DataLoader): the data loader to mix up\n",
    "    alpha (float): value of the parameter to use in the beta distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, dl, alpha):\n",
    "        self.dl, self.alpha = dl, alpha\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for (x, y) in iter(self.dl):\n",
    "            #Taking one different lambda per image speeds up training \n",
    "            lambd = np.random.beta(self.alpha, self.alpha, y.size(0))\n",
    "            #Trick to avoid near duplicates\n",
    "            lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n",
    "            lambd = to_gpu(VV(lambd))\n",
    "            shuffle = torch.randperm(y.size(0))\n",
    "            x1, y1 = x[shuffle], y[shuffle]\n",
    "            yield (x * lambd.view(lambd.size(0),1,1,1) + x1 * (1-lambd).view(lambd.size(0),1,1,1), [y, y1, lambd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapts the loss function to go with mixup.\n",
    "    \n",
    "    Since the targets aren't one-hot encoded, we use the linearity of the loss function with\n",
    "    regards to the target to mix up the loss instead of one-hot encoded targets.\n",
    "    \n",
    "    Argument:\n",
    "    crit: a loss function. It must have the parameter reduced=False to have the loss per element.\n",
    "    \"\"\"\n",
    "    def __init__(self, crit):\n",
    "        super().__init__()\n",
    "        self.crit = crit()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        if not isinstance(target, list): return self.crit(output, target).mean()\n",
    "        loss1, loss2 = self.crit(output,target[0]), self.crit(output,target[1])\n",
    "        return (loss1 * target[2] + loss2 * (1-target[2])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomCrop(sz), RandomFlip()], pad=sz//8)\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_dl = MixUpDataLoader(data.trn_dl, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data)\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "learn.opt_fn = opt_fn\n",
    "learn.data.trn_dl = mixup_dl\n",
    "learn.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90adbd92213b4fc28c4b3e0670f59857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.577392   1.604844   0.4625    \n",
      "    1      1.381691   1.012047   0.6521                     \n",
      "    2      1.285132   0.81694    0.7366                     \n",
      "    3      1.238322   0.772833   0.7486                     \n",
      "    4      1.208034   0.606934   0.8108                     \n",
      "    5      1.155568   0.715823   0.7695                     \n",
      "    6      1.138336   0.585958   0.8217                     \n",
      "    7      1.107028   0.530601   0.8452                     \n",
      "    8      1.094954   0.536645   0.8332                     \n",
      "    9      1.068575   0.53234    0.8518                     \n",
      "    10     1.068159   0.430891   0.869                      \n",
      "    11     1.043203   0.520973   0.8434                     \n",
      "    12     1.051916   0.504943   0.8461                     \n",
      "    13     1.042599   0.451895   0.8745                     \n",
      "    14     1.024971   0.469676   0.8564                     \n",
      "    15     1.002356   0.427404   0.8823                      \n",
      "    16     0.976197   0.459503   0.8596                      \n",
      "    17     0.956913   0.367024   0.8956                      \n",
      "    18     0.940494   0.359177   0.8988                      \n",
      "    19     0.928801   0.356722   0.894                       \n",
      "    20     0.91274    0.361805   0.9037                      \n",
      "    21     0.889095   0.318632   0.913                       \n",
      "    22     0.876805   0.293216   0.9204                      \n",
      "    23     0.858679   0.269319   0.9275                      \n",
      "    24     0.830605   0.239478   0.9366                      \n",
      "    25     0.824642   0.245498   0.9381                      \n",
      "    26     0.805051   0.217718   0.944                       \n",
      "    27     0.784053   0.217338   0.9459                      \n",
      "    28     0.774043   0.20877    0.9492                      \n",
      "    29     0.76365    0.204386   0.95                        \n",
      "\n",
      "CPU times: user 25min 15s, sys: 12min 58s, total: 38min 14s\n",
      "Wall time: 23min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20438589594364165, 0.95]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=0.1, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is as fast as without mixup (23 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second one is with a different alpha: here mixup_dl = MixUpDataLoader(data.trn_dl, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534c42bdc5864eb089ceb754de4dee00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 79/391 [00:08<00:33,  9.32it/s, loss=1.88]\n",
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.472878   1.345358   0.5291    \n",
      "    1      1.269906   0.84233    0.7231                     \n",
      "    2      1.179544   0.839368   0.7125                     \n",
      "    3      1.112833   0.738385   0.7596                     \n",
      "    4      1.055841   0.98404    0.6708                     \n",
      "    5      1.06348    0.654288   0.7998                     \n",
      "    6      1.015946   0.515297   0.8349                     \n",
      "    7      0.996191   0.493962   0.8435                      \n",
      "    8      0.979429   0.525252   0.8424                      \n",
      "    9      0.966817   0.509232   0.8455                      \n",
      "    10     0.957474   0.571616   0.8228                      \n",
      "    11     0.926887   0.495118   0.8441                      \n",
      "    12     0.938132   0.487569   0.8488                      \n",
      "    13     0.930551   0.472688   0.8567                      \n",
      "    14     0.912599   0.442748   0.8683                      \n",
      "    15     0.887375   0.443511   0.869                       \n",
      "    16     0.869825   0.385284   0.881                       \n",
      "    17     0.843568   0.39869    0.8834                      \n",
      "    18     0.83644    0.40036    0.8781                      \n",
      "    19     0.803586   0.317143   0.9071                      \n",
      "    20     0.790489   0.316681   0.9058                      \n",
      "    21     0.782296   0.290825   0.9189                      \n",
      "    22     0.757262   0.280219   0.9208                      \n",
      "    23     0.743585   0.247127   0.9333                      \n",
      "    24     0.709734   0.253216   0.929                       \n",
      "    25     0.698053   0.230306   0.9391                      \n",
      "    26     0.692384   0.209626   0.9426                      \n",
      "    27     0.673137   0.204203   0.9449                      \n",
      "    28     0.653099   0.201423   0.9489                      \n",
      "    29     0.650861   0.193649   0.9493                      \n",
      "\n",
      "CPU times: user 25min 34s, sys: 12min 41s, total: 38min 15s\n",
      "Wall time: 23min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1936487452030182, 0.9493]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=0.1, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular traning with AdamW and using of LogResults callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogResults(Callback):\n",
    "    \"\"\"\n",
    "    Callback to log all the results of the training:\n",
    "    - at the end of each epoch: training loss, validation loss and metrics\n",
    "    - at the end of the first batches then every epoch: deciles of the params and their gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learn, fname, init_text=''):\n",
    "        super().__init__()\n",
    "        self.learn, self.fname, self.init_text = learn, fname, init_text\n",
    "        self.pcts = [0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999]\n",
    "        self.pnames = {p:n for n,p in learn.model.named_parameters()}\n",
    "        self.module_names = get_module_names(learn.model)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.logs, self.epoch, self.n = self.init_text + \"\\n\", 0, 0\n",
    "        self.deciles = {}\n",
    "        for name in self.pnames.values(): \n",
    "            self.deciles[name] = collections.defaultdict(list)\n",
    "            self.deciles[name + '.grad'] = collections.defaultdict(list)\n",
    "        for name in self.module_names.values(): self.deciles[name] = collections.defaultdict(list)\n",
    "        names = [\"epoch\", \"trn_loss\", \"val_loss\", \"metric\"]\n",
    "        layout = \"{!s:10} \" * len(names)\n",
    "        self.logs += layout.format(*names) + \"\\n\"\n",
    "    \n",
    "    def on_batch_begin(self):\n",
    "        if self.n == 0 or (self.epoch == 0 and is_power_of_two(self.n+1)):\n",
    "            self.hooks = []\n",
    "            self.learn.model.apply(self.register_hook)\n",
    "    \n",
    "    def on_batch_end(self, metrics):\n",
    "        self.loss = metrics\n",
    "        if self.n == 0 or (self.epoch == 0 and is_power_of_two(self.n+1)):\n",
    "            self.save_deciles()\n",
    "        if len(self.hooks) != 0:\n",
    "            for h in self.hooks: h.remove()\n",
    "            self.hooks=[]\n",
    "        self.n += 1\n",
    "    \n",
    "    def on_epoch_end(self, metrics):\n",
    "        self.save_stats(self.epoch, [self.loss] + metrics)\n",
    "        self.epoch += 1\n",
    "        self.n=0\n",
    "        \n",
    "    def save_stats(self, epoch, values, decimals=6):\n",
    "        layout = \"{!s:^10}\" + \" {!s:10}\" * len(values)\n",
    "        values = [epoch] + list(np.round(values, decimals))\n",
    "        self.logs += layout.format(*values) + \"\\n\"\n",
    "    \n",
    "    def save_deciles(self):\n",
    "        for group_param in self.learn.sched.layer_opt.opt_params():\n",
    "            for param in group_param['params']:\n",
    "                self.add_deciles(self.pnames[param], to_np(param))\n",
    "                self.add_deciles(self.pnames[param] + '.grad', to_np(param.grad))\n",
    "    \n",
    "    def separate_pcts(self,arr):\n",
    "        n = len(arr.reshape(-1))\n",
    "        pos, neg = arr[arr > 0], arr[arr < 0]\n",
    "        pos_pcts = np.percentile(pos, self.pcts) if len(pos) > 0 else np.array([])\n",
    "        neg_pcts = np.percentile(neg, self.pcts) if len(neg) > 0 else np.array([])\n",
    "        return len(pos)/n, len(neg)/n, pos_pcts, neg_pcts\n",
    "    \n",
    "    def add_deciles(self, name, arr):\n",
    "        pos, neg, pct_pos, pct_neg = self.separate_pcts(arr)\n",
    "        self.deciles[name]['sgn'].append([pos, neg])\n",
    "        self.deciles[name]['pos'].append(pct_pos)\n",
    "        self.deciles[name]['neg'].append(pct_neg)\n",
    "                                                        \n",
    "    def on_train_end(self):\n",
    "        with open(self.fname + '.txt', 'a') as f: f.write(self.logs)\n",
    "        pickle.dump(self.deciles, open(self.fname + '.pkl', 'wb'))\n",
    "        \n",
    "    def register_hook(self, module):\n",
    "        def hook_save_act(module, input, output):\n",
    "            pos, neg, pct_pos, pct_neg = self.separate_pcts(to_np(output))\n",
    "            m_name = self.module_names[module]\n",
    "            self.deciles[m_name]['sgn'].append([pos, neg])\n",
    "            self.deciles[m_name]['pos'].append(pct_pos)\n",
    "            self.deciles[m_name]['neg'].append(pct_neg)\n",
    "        if (not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == self.learn.model)):\n",
    "            self.hooks.append(module.register_forward_hook(hook_save_act))\n",
    "\n",
    "def get_module_names(model):\n",
    "    def register_names(module):\n",
    "        if (not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model)):\n",
    "            class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n",
    "            m_name = f'{class_name}-{len(names)+1}'\n",
    "            names[module] = m_name\n",
    "    names = {}\n",
    "    model.apply(register_names)\n",
    "    return names\n",
    "\n",
    "def is_power_of_two(n):\n",
    "    while n>1:\n",
    "        if n%2 != 0: return False\n",
    "        n = n//2\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomCrop(sz), RandomFlip()], pad=sz//8)\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cb = LogResults(learn, str(PATH/'cifar10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736336f88e444b049c814aea37b8d588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.099058   1.092296   0.6064    \n",
      "    1      0.840501   0.879823   0.6933                      \n",
      "    2      0.691204   0.750623   0.7482                      \n",
      "    3      0.617986   0.590799   0.8001                      \n",
      "    4      0.553649   0.562221   0.808                       \n",
      "    5      0.503341   0.857572   0.7261                      \n",
      "    6      0.460281   0.526698   0.8185                      \n",
      "    7      0.440226   0.519977   0.8228                      \n",
      "    8      0.413289   0.56235    0.8171                      \n",
      "    9      0.393606   0.447091   0.8463                      \n",
      "    10     0.370249   0.484564   0.8476                      \n",
      "    11     0.353899   0.442357   0.8554                      \n",
      "    12     0.34476    0.385429   0.8705                      \n",
      "    13     0.343156   0.456771   0.8485                      \n",
      "    14     0.304945   0.429652   0.8579                      \n",
      "    15     0.274984   0.33127    0.8914                      \n",
      "    16     0.253462   0.325269   0.8898                      \n",
      "    17     0.227426   0.382997   0.8765                      \n",
      "    18     0.209817   0.299588   0.8988                      \n",
      "    19     0.194897   0.293427   0.9044                      \n",
      "    20     0.169136   0.291551   0.9095                      \n",
      "    21     0.148512   0.308182   0.904                       \n",
      "    22     0.132138   0.256267   0.9215                      \n",
      "    23     0.112354   0.259355   0.922                        \n",
      "    24     0.08252    0.253447   0.923                        \n",
      "    25     0.062529   0.237996   0.9301                       \n",
      "    26     0.044835   0.221493   0.9358                       \n",
      "    27     0.0276     0.231811   0.9398                       \n",
      "    28     0.01912    0.236096   0.9399                       \n",
      "    29     0.012238   0.226691   0.9409                       \n",
      "\n",
      "CPU times: user 44min 47s, sys: 13min 45s, total: 58min 33s\n",
      "Wall time: 26min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22669122726917268, 0.9409]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=0.1, use_wd_sched=True, callbacks=[log_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loggging the results affects performance a bit (especially in the first epoch where we log results every 2^n batch), here we go from 23 minutes to 26min 36s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
